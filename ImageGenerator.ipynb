{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as fn\n",
    "import math\n",
    "import random as r\n",
    "import math as m\n",
    "\n",
    "from cleanfid import fid\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from einops import rearrange, reduce, asnumpy, parse_shape, repeat, einsum\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "import shutil\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Device is {device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tv.transforms.Compose([\n",
    "    tv.transforms.RandomHorizontalFlip(0.5),\n",
    "    tv.transforms.ToTensor(), # turn into torch Tensor of shape CHW, divide by 255\n",
    "    tv.transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "transform_verify = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "dataset = tv.datasets.CIFAR100(root=\"./Datasets\", train=True, transform=transform, download=True)\n",
    "dataset_verify = tv.datasets.CIFAR100(root=\"./Datasets\", train=False, transform=transform_verify, download=True)\n",
    "\n",
    "num_classes = 100\n",
    "image_channels = 3\n",
    "batch_size = 256\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "dataloader_verify = torch.utils.data.DataLoader(dataset_verify, batch_size=batch_size, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images in dataloader:\n",
    "    print(len(images))\n",
    "    print(images[0].shape)\n",
    "    print(images[1].shape)\n",
    "    \n",
    "    print(images[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in dataloader:\n",
    "    print(images[0].shape)\n",
    "    \n",
    "    for x in range(1, 13):\n",
    "        plt.subplot(2, 6, x)\n",
    "        plt.imshow(images[x].permute(1, 2, 0), cmap=plt.cm.binary)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(dataloader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "T = 400\n",
    "\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "\n",
    "def cosine_schedule(timesteps, s=0.008):\n",
    "    def f(t):\n",
    "        return torch.cos((t / timesteps + s) / (1 + s) * 0.5 * torch.pi) ** 2\n",
    "    x = torch.linspace(0, timesteps, timesteps + 1)\n",
    "    alphas_cumprod = f(x) / f(torch.tensor([0]))\n",
    "    betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "    betas = torch.clip(betas, 0.0001, 0.999)\n",
    "    return betas\n",
    "\n",
    "\n",
    "def linear_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
    "\n",
    "#Code taken from https://huggingface.co/blog/annotated-diffusion\n",
    "# and https://dzdata.medium.com/intro-to-diffusion-model-part-4-62bd94bd93fd\n",
    "\n",
    "betas = sigmoid_beta_schedule(timesteps=T)\n",
    "\n",
    "# define alphas \n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = fn.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "def sample_images(image, ts, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(image)\n",
    "    return sqrtab[ts] * image + sqrtmab[ts] * noise\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "\n",
    "reverse_transform = tv.transforms.Compose([\n",
    "     tv.transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "     tv.transforms.Lambda(lambda t: t * 255.),\n",
    "     tv.transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "     tv.transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "     tv.transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "def get_noisy_image(x_start, t):\n",
    "    # add noise\n",
    "    x_noisy = q_sample(x_start, t=t)\n",
    "\n",
    "    # turn back into PIL image\n",
    "    noisy_image = reverse_transform(x_noisy.squeeze())\n",
    "\n",
    "    return noisy_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [image] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "load_iter = iter(dataloader)\n",
    "images, _ = next(load_iter)\n",
    "    \n",
    "plot([get_noisy_image(images[0], torch.tensor([t])) for t in [0, 50, 100, 150, 200, 250, 300, 350]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code partially taken from https://huggingface.co/blog/annotated-diffusion\n",
    "#way to inject conditionals taken from https://github.com/TeaPearce/Conditional_Diffusion_MNIST\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_layers):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "\n",
    "        self.in_layers = in_layers\n",
    "        self.convolutions = nn.Sequential(nn.Conv2d(in_layers, in_layers, 3, 1, 1, groups=in_layers),\n",
    "                                           nn.BatchNorm2d(in_layers),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Conv2d(in_layers, in_layers, 1, 1),\n",
    "                                           nn.BatchNorm2d(in_layers),\n",
    "                                           nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.convolutions(x)\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_layers, bottleneck):\n",
    "        super(Inception, self).__init__()\n",
    "\n",
    "        self.in_layers = in_layers\n",
    "        self.bottleneck = bottleneck\n",
    "        self.bn = nn.BatchNorm2d(in_layers)\n",
    "        \n",
    "        self.layers = nn.Sequential(nn.Conv2d(in_layers, bottleneck, 1, 1),\n",
    "                                    nn.BatchNorm2d(bottleneck),\n",
    "                                    nn.ReLU(),\n",
    "                                    DepthwiseSeparableConv(bottleneck),\n",
    "                                    nn.Conv2d(bottleneck, in_layers, 1, 1),\n",
    "                                    nn.GroupNorm(8, in_layers),\n",
    "                                    nn.ReLU())\n",
    "                                    \n",
    "    def forward(self, x):\n",
    "        x = x + self.layers(x)\n",
    "        return self.bn(x)\n",
    "    \n",
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, layers_num, bottleneck_size):\n",
    "        super(ResLayer, self).__init__()\n",
    "        self.layers_num = layers_num\n",
    "\n",
    "        self.layers = nn.Sequential(Inception(layers_num, bottleneck_size),\n",
    "                                    Inception(layers_num, bottleneck_size),\n",
    "                                    Inception(layers_num, bottleneck_size),\n",
    "                                    Inception(layers_num, bottleneck_size),\n",
    "                                    Inception(layers_num, bottleneck_size),\n",
    "                                    Inception(layers_num, bottleneck_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_layers, bottleneck_size, out_layers, add_attention=False):\n",
    "        super(UNetBlock, self).__init__()\n",
    "\n",
    "        self.in_layers = in_layers\n",
    "        self.out_layers = out_layers\n",
    "      \n",
    "        self.layers = ResLayer(in_layers, bottleneck_size)\n",
    "        \n",
    "        self.group_normilisation = nn.GroupNorm(8, in_layers)\n",
    "        \n",
    "        self.bottleneck_out = nn.Conv2d(in_layers, out_layers, 1, 1)\n",
    "\n",
    "        self.attention = None\n",
    "        if add_attention:\n",
    "            self.attention = Attention(self.out_layers, 4, self.out_layers//4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.group_normilisation(x)\n",
    "        \n",
    "        if self.attention:\n",
    "            out = self.attention(out)\n",
    "            \n",
    "        return self.bottleneck_out(x)\n",
    "\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self, in_layers, out_layers):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        self.convolution = nn.Sequential(nn.Conv2d(in_layers, out_layers, 1, 1),\n",
    "                                         nn.BatchNorm2d(out_layers),\n",
    "                                         nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolution(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_layers, out_layers, bottleneck_size, add_attention=False):\n",
    "        super(Down, self).__init__()\n",
    "        self.convolutions = UNetBlock(in_layers, bottleneck_size, out_layers, add_attention)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        downsampled = fn.max_pool2d(x, 2, 2)\n",
    "        return downsampled, x\n",
    "\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_layers, out_layers, bottleneck_size, add_attention=False):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        self.upsampler = nn.ConvTranspose2d(out_layers, out_layers, 2, 2)\n",
    "        self.convolutions = UNetBlock(in_layers, bottleneck_size, out_layers, add_attention)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        if skip is not None:\n",
    "            x = torch.concat([x, skip], dim=1)\n",
    "        x = self.convolutions(x)\n",
    "        return self.upsampler(x)\n",
    "    \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        layer1 = 64\n",
    "        layer2 = 128\n",
    "        layer3 = 184\n",
    "\n",
    "        out = 72\n",
    "\n",
    "        self.entry = nn.Conv2d(image_channels, 32, 1, 1)\n",
    "\n",
    "        self.ConvDown1 = Down(32, layer1, 32)\n",
    "        self.ConvDown2 = Down(layer1, layer2, 64)\n",
    "        self.ConvDown3 = Down(layer2, layer3, 92)\n",
    "\n",
    "        self.downsampler = nn.AvgPool2d(4)\n",
    "        \n",
    "        self.middle_to_up = nn.Sequential(nn.ConvTranspose2d(layer3, layer3, 2, 1),\n",
    "                                          nn.GroupNorm(8, layer3),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.ConvTranspose2d(layer3, layer3, 2, 2),\n",
    "                                          nn.GroupNorm(8, layer3),\n",
    "                                          nn.ReLU())\n",
    "        \n",
    "        self.ConvUp3 = Up(layer3, layer2//2, 40)\n",
    "        self.ConvUp2 = Up(layer2, layer1//2, 22)\n",
    "        self.ConvUp1 = Up(layer1, out//2, 12)\n",
    "\n",
    "        self.final = nn.Sequential(nn.Conv2d(out, image_channels, 1, 1))\n",
    "    \n",
    "        self.squeezer1 = SkipConnection(layer1, out//2)\n",
    "        self.squeezer2 = SkipConnection(layer2, layer1//2)\n",
    "        self.squeezer3 = SkipConnection(layer3, layer2//2)\n",
    "        \n",
    "        \n",
    "        self.t_encoder1 = nn.Sequential(nn.Linear(1, 180),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(180, layer2),\n",
    "                                        nn.BatchNorm1d(layer2))\n",
    "        \n",
    "        self.label_encoder1 = nn.Sequential(nn.Linear(num_classes, 180),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(180, layer2),\n",
    "                                    nn.BatchNorm1d(layer2))\n",
    "        \n",
    "        self.t_encoder2 = nn.Sequential(nn.Linear(1, 180),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(180, layer3),\n",
    "                                        nn.BatchNorm1d(layer3))\n",
    "        \n",
    "        self.label_encoder2 = nn.Sequential(nn.Linear(num_classes, 180),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(180, layer3),\n",
    "                                    nn.BatchNorm1d(layer3))\n",
    "        \n",
    "    \n",
    "    def forward(self, x, labels, t):\n",
    "        \n",
    "        ch = fn.one_hot(labels, num_classes=num_classes).to(device)\n",
    "        ch = ch.to(torch.float32)\n",
    "        \n",
    "        label_encoding2 = self.label_encoder2(ch)\n",
    "        label_encoding1 = self.label_encoder1(ch)\n",
    "        \n",
    "        \n",
    "        t = t.float()\n",
    "        t = torch.unsqueeze(t, dim=1)\n",
    "        t_encoding2 = self.t_encoder2(t)\n",
    "        t_encoding1 = self.t_encoder1(t)\n",
    "\n",
    "        x, skip1 = self.ConvDown1(self.entry(x))\n",
    "        x, skip2 = self.ConvDown2(x)\n",
    "        x, skip3 = self.ConvDown3(x)\n",
    "        \n",
    "        \n",
    "        hidden_vec = self.downsampler(x)\n",
    "        \n",
    "        x = self.middle_to_up(hidden_vec)\n",
    "        label_encoding2 = repeat(label_encoding2, \"b c -> b c w h\", w=x.shape[2], h=x.shape[3])\n",
    "        t_encoding2 = repeat(t_encoding2, \"b c -> b c w h\", w=x.shape[2], h=x.shape[3])\n",
    "        \n",
    "        x = self.ConvUp3(x*label_encoding2 + t_encoding2)\n",
    "        \n",
    "        x = torch.concat([x, self.squeezer3(skip3)], dim=1)\n",
    "        label_encoding1 = repeat(label_encoding1, \"b c -> b c w h\", w=x.shape[2], h=x.shape[3])\n",
    "        t_encoding1 = repeat(t_encoding1, \"b c -> b c w h\", w=x.shape[2], h=x.shape[3])\n",
    "        x = self.ConvUp2(x*label_encoding1 + t_encoding1)\n",
    "        x = self.ConvUp1(x, self.squeezer2(skip2))\n",
    "        \n",
    "        return self.final(torch.concat([x, self.squeezer1(skip1)], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"ModelSave/linear_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=3):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.net = UNet()\n",
    "    \n",
    "    def interpolate(self):\n",
    "        num_pairs = 8\n",
    "        \n",
    "        labels = torch.randint(low=0, high=num_classes, size=(num_pairs,))\n",
    "        image_pairs = []\n",
    "        \n",
    "        for i in range(num_pairs//2):\n",
    "            image_pairs.append((self.p_sample_loop(2, labels[2*i:2*i+2]), labels[2*i:2*i+2]))\n",
    "            \n",
    "        interpolated_images = []\n",
    "        \n",
    "        for ([x1, x2], [label1, label2]) in image_pairs:\n",
    "            interpolated_images.append([x1] + self.interpolate_pair(x1, x2, i, label1, label2) + [x2]) \n",
    "        \n",
    "        return interpolated_images\n",
    "    \n",
    "    def interpolate_simple(self):\n",
    "        num_pairs = 8\n",
    "        \n",
    "        latent_vectors = torch.randn(16, 3, 32, 32).to(device)\n",
    "        result = []\n",
    "        for i in range(0, 16, 2):\n",
    "            vec1 = latent_vectors[i]\n",
    "            vec2 = latent_vectors[i+1]\n",
    "            \n",
    "            vec1_b = torch.unsqueeze(vec1, dim=0)\n",
    "            vec2_b = torch.unsqueeze(vec2, dim=0)\n",
    "            \n",
    "            random_label = torch.randint(low=0, high=100, size=(1,)).item()\n",
    "\n",
    "            # Create a PyTorch tensor with the specified properties\n",
    "            tensor_length = 8\n",
    "            interpolation_labels = torch.cat([torch.full((4,), random_label), torch.randint(low=0, high=100, size=(4,))])\n",
    "\n",
    "            interpolations = [vec1_b]\n",
    "            for ii in range(1, 7):\n",
    "                interpolation = torch.lerp(vec1, vec2, ii/7)\n",
    "                interpolation = torch.unsqueeze(interpolation, dim=0)\n",
    "                interpolations.append(interpolation)\n",
    "            interpolations.append(vec2_b)\n",
    "            \n",
    "            interpolations = torch.concat(interpolations, dim=0)\n",
    "            \n",
    "            \n",
    "            interpolations = self.p_sample_loop(8, interpolation_labels, interpolations)\n",
    "            result = [*result, *interpolations]\n",
    "            \n",
    "        return result\n",
    "            \n",
    "        \n",
    "    def interpolate_pair(self, x1, x2, t, label1, label2):\n",
    "        schedule = [50, 100, 200, 200, 100, 50]\n",
    "        output = []\n",
    "        \n",
    "        for i, t in enumerate(schedule):\n",
    "            ts = torch.tensor([t, t], dtype=torch.int64).to(device)\n",
    "            noise = torch.randn(3, 32, 32).to(device)\n",
    "            noise = repeat(noise, \"c w h -> b c w h\", b=2)\n",
    "            x1 = torch.unsqueeze(x1, dim=0)\n",
    "            x2 = torch.unsqueeze(x2, dim=0)\n",
    "            pair = torch.concat([x1, x2], dim=0).to(device)\n",
    "            sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, ts, pair.shape)\n",
    "            sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "                sqrt_one_minus_alphas_cumprod, ts, pair.shape\n",
    "            )\n",
    "            \n",
    "            print(sqrt_alphas_cumprod_t.device)\n",
    "            print(pair.device)\n",
    "            print(sqrt_one_minus_alphas_cumprod_t.device)\n",
    "            print(noise.device)\n",
    "            \n",
    "            \n",
    "            diffused_xs = sqrt_alphas_cumprod_t * pair + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "            x1 = diffused_xs[0]\n",
    "            x2 = diffused_xs[1]\n",
    "            latent_z = x1 * (((1/x1)*x2)**(i/len(schedule)))\n",
    "            latent_label = label1 if i < 3  else label2\n",
    "            print(latent_z.shape)\n",
    "            latent_z = torch.unsqueeze(latent_z, dim=0)\n",
    "            latent_label = torch.unsqueeze(latent_label, dim=0)\n",
    "            for i in reversed(range(0, t)):\n",
    "                latent_z = self.p_sample(latent_z, latent_label, torch.full((1,), i, device=device, dtype=torch.long), i)\n",
    "            output.append(latent_z)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        ts = torch.randint(0, T, (x.shape[0],)).to(x.device)\n",
    "        eps = torch.randn_like(x)\n",
    "        noisy_image = q_sample(x, ts, eps)\n",
    "        noise_prediction = self.net(noisy_image, labels, ts)\n",
    "        return fn.mse_loss(eps, noise_prediction)\n",
    "    \n",
    "    def p_sample(self, x, labels, t, t_index):\n",
    "        betas_t = extract(betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "        sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "        model_mean = sqrt_recip_alphas_t * (x - betas_t * self.net(x, labels, t) / sqrt_one_minus_alphas_cumprod_t)\n",
    "        \n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "            noise = torch.randn_like(x)\n",
    "            \n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "    \n",
    "    def p_sample_loop(self, num_samples, labels_in=None, noise_batch=None):\n",
    "        shape = [num_samples, 3, 32, 32]\n",
    "        \n",
    "        if labels_in is None:\n",
    "            random_labels = torch.randint(low=0, high=num_classes, size=(num_samples,))\n",
    "        else:\n",
    "            random_labels = labels_in\n",
    "        \n",
    "        b = shape[0]\n",
    "        # start from pure noise (for each example in the batch)\n",
    "        if noise_batch is None:\n",
    "            img = torch.randn(shape, device=device)\n",
    "        else:\n",
    "            img = noise_batch\n",
    "        imgs = []\n",
    "\n",
    "        for i in reversed(range(0, T)):\n",
    "            img = self.p_sample(img, random_labels, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "        return img\n",
    "    \n",
    "    def sample(self, noise_batch):\n",
    "        num_samples = noise_batch.shape[0]\n",
    "        random_labels = torch.randint(low=0, high=num_classes, size=(num_samples,))\n",
    "        \n",
    "        x = noise_batch\n",
    "\n",
    "        for t in range(T-1, -1, -1):\n",
    "            z = torch.randn(num_samples, 3, 32, 32).to(device) if t > 0 else 0\n",
    "            ts = torch.tensor(t).repeat(num_samples).to(device)\n",
    "            noise_prediction = self.net(x, random_labels, ts)\n",
    "            x = oneover_sqrta[t] * (x - (noise_prediction * beta_t[t] / sqrtmab[t])) + sigma_t[t] * z\n",
    "        return x\n",
    "    \n",
    "model = DDPM()\n",
    "model = model.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"diffusion_model_final\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    xh = model.p_sample_loop(10)\n",
    "img_grid = rearrange(xh, '(b1 b2) c h w -> (b1 h) (b2 w) c', b1=2)\n",
    "plt.imshow((img_grid.cpu()*255).int().numpy())\n",
    "plt.show()\n",
    "\n",
    "del xh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Parameter Number: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(\"ModelSave/best_model\"))\n",
    "#model_save_path = \"ModelSave/model_best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"ModelSave/best_model\"\n",
    "\n",
    "epochs = 3000\n",
    "epoch = 0\n",
    "train_batches_count = 2000\n",
    "\n",
    "step_count = 0\n",
    "min_loss = 10.0\n",
    "do_continue = True\n",
    "\n",
    "while do_continue:\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    train_iter = iter(dataloader)\n",
    "    for i in range(train_batches_count):\n",
    "        images, labels = next(train_iter, (None, None))\n",
    "        if images is None:\n",
    "            break\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        loss = model(images, labels)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        step_count += 1\n",
    "        \n",
    "        if step_count >= 100000:\n",
    "            do_continue = False\n",
    "            break\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        for i, batch in enumerate(dataloader_verify):\n",
    "            x, labels = batch\n",
    "            x, labels = x.to(device), labels.to(device)\n",
    "\n",
    "            loss = model(x, labels)\n",
    "            test_losses.append(loss.item())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            sample_count = 8\n",
    "            noise_batch = torch.rand((sample_count, 3, 32, 32)).to(device)\n",
    "            \n",
    "            xh = model.p_sample_loop(sample_count)\n",
    "            img_grid = rearrange(xh, 'b c h w -> h (b w) c')\n",
    "            img_grid = (img_grid + 1) / 2\n",
    "            plt.imshow((img_grid.cpu()*255).int().numpy())\n",
    "            plt.show()\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    average_loss = sum(losses)/len(losses)\n",
    "    if average_loss < min_loss:\n",
    "        min_loss = average_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        \n",
    "    scheduler.step()\n",
    "    print(f\"At epoch {epoch}: average loss {average_loss}, average test loss {sum(test_losses)/len(test_losses)}\")\n",
    "    print(f\"Num Steps: {step_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images_dir = 'real_images'\n",
    "generated_images_dir = 'generated_images'\n",
    "num_samples = 10000\n",
    "\n",
    "# create/clean the directories\n",
    "def setup_directory(directory):\n",
    "    if os.path.exists(directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "setup_directory(real_images_dir)\n",
    "setup_directory(generated_images_dir)\n",
    "\n",
    "# generate and save 10k model samples\n",
    "num_generated = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    while num_generated < num_samples:\n",
    "\n",
    "        # sample from model\n",
    "        samples_batch = model.p_sample_loop(100).cpu().detach()\n",
    "\n",
    "        for image in samples_batch:\n",
    "            if num_generated >= num_samples:\n",
    "                break\n",
    "            save_image(image, os.path.join(generated_images_dir, f\"gen_img_{num_generated}.png\"))\n",
    "            num_generated += 1\n",
    "        \n",
    "        if num_generated % 1000:\n",
    "            print(f\"{num_generated} images generated!!!\")\n",
    "            \n",
    "transform_fid = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "])\n",
    "\n",
    "dataset_fid = tv.datasets.CIFAR100(root=\"./Datasets\", train=False, transform=transform_fid, download=True)\n",
    "dataloader_fid = torch.utils.data.DataLoader(dataset_verify, batch_size=1, drop_last=True, shuffle=True)\n",
    "\n",
    "# save 10k images from the CIFAR-100 test dataset\n",
    "num_saved_real = 0\n",
    "test_iter = iter(dataloader_fid)\n",
    "while num_saved_real < num_samples:\n",
    "    real_samples_batch, _ = next(test_iter, (None, None))\n",
    "    if real_samples_batch is None:\n",
    "        break\n",
    "    for image in real_samples_batch:\n",
    "        if num_saved_real >= num_samples:\n",
    "            break\n",
    "        save_image(image, os.path.join(real_images_dir, f\"real_img_{num_saved_real}.png\"))\n",
    "        num_saved_real += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = fid.compute_fid(real_images_dir, generated_images_dir, mode=\"clean\", num_workers=0)\n",
    "print(f\"FID score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_count = 64\n",
    "    xh = model.p_sample_loop(sample_count)\n",
    "    img_grid = rearrange(xh, '(b1 b2) c h w -> (b1 h) (b2 w) c', b1=8)\n",
    "    img_grid = (img_grid + 1) / 2\n",
    "    plt.imshow((img_grid.cpu()*255).int().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    interpolated_rows = model.interpolate_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list_np = [((img+1)/2).cpu().numpy() for img in interpolated_rows]\n",
    "\n",
    "# Display the tensors in an 8x8 grid using Matplotlib\n",
    "fig, axs = plt.subplots(8, 8, figsize=(16, 16))\n",
    "\n",
    "for i in range(8):\n",
    "    for ii in range(8):\n",
    "        index = i * 8 + ii\n",
    "        axs[i, j].imshow(tensor_list_np[index].transpose(1, 2, 0))  # Transpose to (H, W, C) for RGB images\n",
    "        axs[i, j].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearningcw_kernel_3",
   "language": "python",
   "name": "deeplearningcw_kernel_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
